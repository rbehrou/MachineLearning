{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e2ee1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cde725",
   "metadata": {},
   "source": [
    "# Sigmoid (Logistic) Function\n",
    "\n",
    "For classification tasks, one can start using a linear regression model (i.e. $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b$), to predict $y$ for a given $x$. However, the linear regression model will fail to predict the output variable $y$ to be either $0$ or $1$, used for the classification purposes. This can be accomplished by using a \"sigmoid function\" which maps all input values to values between $0$ and $1$. \n",
    "\n",
    "The formula for a sigmoid function is given as follows: \n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}} \\nonumber$$\n",
    "\n",
    "In the case of logistic regression, $z$ (the input to the sigmoid function), is the output of the linear regression model.\n",
    "\n",
    "- In the case of a single dataset, $z$ is scalar.\n",
    "- in the case of multiple datasets, $z$ may be a vector consisting of $m$ values, one for each dataset. \n",
    "\n",
    "The `sigmoid` function is implemented as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e780f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "    g = 1.0/(1.0+np.exp(-z))\n",
    "   \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7193980",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "A logistic regression model applies the sigmoid function to the linear regression model as follows:\n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\nonumber $$ \n",
    "\n",
    " where $g(\\mathbf{z})$ is defined as:\n",
    "\n",
    " $$g(\\mathbf{z}) = \\frac{1}{1+e^{-\\mathbf{z}}} \\nonumber$$\n",
    " \n",
    " with\n",
    " \n",
    " $$\\mathbf{z} = \\mathbf{w} \\cdot \\mathbf{x} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4c87d",
   "metadata": {},
   "source": [
    "# Decision Boundary in Logestic Regession\n",
    "  \n",
    "One can interpret the output of the model ($f_{\\mathbf{w},b}(\\mathbf{x})$) as the probability that $y=1$ given $\\mathbf{x}$ and parameterized by $\\mathbf{w}$ and $b$.\n",
    "\n",
    "Therefore, to get a final prediction ($y = 0$ or $y = 1$) from the logistic regression model, one can use the following heuristic:\n",
    "\n",
    "- if $f_{\\mathbf{w},b}(x) >= 0.5$, predict $y=1$\n",
    "  \n",
    "- if $f_{\\mathbf{w},b}(x) < 0.5$, predict $y=0$\n",
    "  \n",
    "  \n",
    "If we plot the sigmoid function we can see that for $z >=0$, $g(z) >= 0.5$ and for $z < 0$, $g(z) < 0.5$. \n",
    "\n",
    "* Therefore, for a logistic regression model where $\\mathbf{z} = \\mathbf{w} \\cdot \\mathbf{x} + b$:\n",
    "\n",
    "  if $\\mathbf{w} \\cdot \\mathbf{x} + b >= 0$, the model predicts $y=1$\n",
    "  \n",
    "  if $\\mathbf{w} \\cdot \\mathbf{x} + b < 0$, the model predicts $y=0$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "045a859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'z')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sigmoid(z) over a range of values from -10 to 10\n",
    "z = np.arange(-10,11)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,3))\n",
    "# Plot z vs sigmoid(z)\n",
    "ax.plot(z, sigmoid(z), c=\"b\")\n",
    "ax.plot([0.0, 0.0], [0.0, 1.0], c=\"r\")\n",
    "\n",
    "ax.set_title(\"Sigmoid function\")\n",
    "ax.set_ylabel('sigmoid(z)')\n",
    "ax.set_xlabel('z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31e9fb",
   "metadata": {},
   "source": [
    "# Logistic Loss Function\n",
    "\n",
    "Recall that for **Linear** Regression we have used the **squared error cost function**. The squared error cost function had the nice property that following the derivative of the cost leads to the minimum. While this cost function works well for linear regression, in logestic regression $f_{wb}(x)$ has a non-linear component (i.e. the sigmoid function $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b )$). Therefore, logistic regression requires a cost function more suitable to its non-linear nature. This can be defined as a **loss function** as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$} \\nonumber\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "where $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point with:\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ denotes the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$, where $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combining these curves provides a behavior that is useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider these two curves, the cost function for the logistic regression forms as follows:\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\nonumber $$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point given as:\n",
    "\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\nonumber$$\n",
    "    \n",
    "Note that $m$ is the number of training examples in the data set and:\n",
    "\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)}) \\nonumber \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b \\nonumber \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}} \\nonumber \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The algorithm for `compute_logistic_loss` loops over all the examples calculating the loss for each example and accumulating the total.\n",
    "\n",
    "Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($𝑚$,) respectively, where  $𝑛$ is the number of features and $𝑚$ is the number of training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79bc1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_loss(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes logestic loss\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      loss (scalar): loss\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    loss = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        loss +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    loss = loss / m\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a54083",
   "metadata": {},
   "source": [
    "# Gradient Descent for Logistic Regression\n",
    "\n",
    "Recall the gradient descent algorithm utilizes the gradient calculation as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "w_j &= w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\qquad \\forall j = 0, \\dots, n-1 \\nonumber \\\\ \n",
    "b &= b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\nonumber \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where at each iteration we perform simultaneous updates on $w_j$ for all $j$, where\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\nonumber \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\nonumber \n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "* m is the number of training examples in the data set      \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n",
    "* For a logistic regression model  \n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    and $g(z)$ is the sigmoid function:  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
    "    \n",
    "The codes for the gradient descent are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8edc5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_dw, dj_db  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7d499b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient decent method\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, rel_err):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    Args:\n",
    "        X (ndarray (m,n): Data, m examples with n features\n",
    "        y (ndarray (m,)): target values\n",
    "        w_in (ndarray (n,)) : initial model parameters\n",
    "        b_in (scalar): initial model parameter\n",
    "        alpha (float): learning rate\n",
    "        num_iters (int): number of iterations to run gradient descent\n",
    "        rel_err(float): relative error in the gradient decent\n",
    "    Returns:\n",
    "        w (scalar): Updated value of parameter after running gradient descent\n",
    "        b (scalar): Updated value of parameter after running gradient descent\n",
    "        J_history (List): History of cost values\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    b         = b_in\n",
    "    w         = w_in\n",
    "    rel_diff  = 1.0\n",
    "    i         = 0\n",
    "    \n",
    "    # Loop over number of iterations\n",
    "    while (i < num_iters) or (rel_diff > rel_err):\n",
    "\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = compute_gradient_logistic(X, y, w, b)\n",
    "        \n",
    "        # Update Parameters using equation for the gradient decent\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000: # prevent resource exhaustion\n",
    "            J_history.append(compute_logistic_loss(X, y, w, b))\n",
    "        \n",
    "        # Relative difference\n",
    "        if i > 0:\n",
    "            rel_diff = abs(J_history[i]-J_history[i-1])/J_history[i]\n",
    "        \n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Itr {i:4}: Cost = {J_history[-1]:8.9f}, rel_diff = {rel_diff:0.9e}\")\n",
    "        \n",
    "        # Update\n",
    "        i += 1\n",
    "        \n",
    "    # return w and J,w history for graphing\n",
    "    print(f\"Itr {i:4}: Cost = {J_history[-1]:8.9f}, rel_diff = {rel_diff:0.9e}\")\n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9eb0aa",
   "metadata": {},
   "source": [
    "# Run an Example using in-house Logestic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2af87b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr    0: Cost = 0.684610469, rel_diff = 1.000000000e+00\n",
      "Itr 1000: Cost = 0.159097767, rel_diff = 8.535521413e-04\n",
      "Itr 2000: Cost = 0.084600642, rel_diff = 4.781698964e-04\n",
      "Itr 3000: Cost = 0.057053273, rel_diff = 3.282283690e-04\n",
      "Itr 4000: Cost = 0.042907594, rel_diff = 2.489516787e-04\n",
      "Itr 5000: Cost = 0.034338477, rel_diff = 2.001970148e-04\n",
      "Itr 6000: Cost = 0.028603798, rel_diff = 1.672708979e-04\n",
      "Itr 7000: Cost = 0.024501570, rel_diff = 1.435756908e-04\n",
      "Itr 8000: Cost = 0.021423703, rel_diff = 1.257221998e-04\n",
      "Itr 9000: Cost = 0.019030137, rel_diff = 1.117949719e-04\n",
      "Itr 10000: Cost = 0.017117769, rel_diff = 1.006414383e-04\n",
      "(J, w, b) found by gradient descent: (0.0171178, [5.28123029 5.07815608], -14.2224100)\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    # dataset\n",
    "    X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "    y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "    # initialize parameters\n",
    "    w_init = np.zeros_like(X_train[0])\n",
    "    b_init = 0.0\n",
    "\n",
    "    # some gradient descent settings\n",
    "    num_iters = 10000\n",
    "    alpha     = 1.0e-1\n",
    "    rel_err   = 1.0e-2\n",
    "    \n",
    "    # run gradient descent\n",
    "    w_final, b_final, J_hist = gradient_descent(X_train, y_train, w_init, b_init, alpha, num_iters, rel_err)\n",
    "    print(f\"(J, w, b) found by gradient descent: ({J_hist[-1]:0.7f}, {w_final}, {b_final:0.7f})\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    # reference parameters: w:[5.28 5.08], b:-14.222409982019837\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f3fe1",
   "metadata": {},
   "source": [
    "# Run the same example using Scikit-Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "13e0030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on training set: [0 0 0 1 1 1]\n",
      "Accuracy on training set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# call logestic regression model\n",
    "lr_model = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "# fit the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# make prediction\n",
    "y_pred = lr_model.predict(X_train)\n",
    "print(\"Prediction on training set:\", y_pred)\n",
    "# y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# calculate accuracy\n",
    "print(\"Accuracy on training set:\", lr_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9182df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

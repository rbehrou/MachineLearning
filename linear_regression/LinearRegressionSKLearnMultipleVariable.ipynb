{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff53215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9940c",
   "metadata": {},
   "source": [
    "# Scikit-learn Gradient Descent\n",
    "Scikit-learn has a gradient descent regression model [sklearn.linear_model.SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#examples-using-sklearn-linear-model-sgdregressor).  Like the gradient descent implementation as shown below, this model performs best with normalized inputs. [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) will perform z-score normalization as shown below. Here it is referred to as 'standard score'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43f6ca",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "In linear regression with multiple variables, the cost function is a measure on how well our model\n",
    "is predicting the target values.\n",
    "\n",
    "The equation for cost function with multiple variable is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\nonumber $$ \n",
    " \n",
    "where \n",
    "  $$f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\nonumber $$\n",
    "  \n",
    "In contrast to the cost function with single variable, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors supporting multiple features.\n",
    "  \n",
    "- $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the prediction for sample $i$ using parameters $\\mathbf{w},b$.  \n",
    "- $(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.   \n",
    "- These differences are summed over all the $m$ samples and divided by $2m$ to produce the cost, $J(\\mathbf{w},b)$.  \n",
    "- Note that the summation ranges from 0 to m-1 (compatible with python indexing).\n",
    "\n",
    "Here is the code to compute the cost function with multiple variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2c72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function calculator\n",
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression with multiple variables\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters \n",
    "      b (scalar)       : model parameter\n",
    "    Returns\n",
    "      cost (scalar): The total cost\n",
    "     \"\"\"\n",
    "   \n",
    "    # initialize\n",
    "    m = X.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    # loop over number of data sets\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(X[i], w) + b\n",
    "        cost += (f_wb - y[i])**2\n",
    "    # total cost\n",
    "    cost = cost / (2 * m)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f741b4a",
   "metadata": {},
   "source": [
    "# Gradient of the cost function\n",
    "From the cost function equation, the gradient of the cost function with respect to $w_{j}$ and $b$ is given\n",
    "as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x^{(i)}_{j} \\qquad \\forall j = 0, \\dots, n-1 \\nonumber \\\\\n",
    "  \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\nonumber \\\\\n",
    "\\end{align}\n",
    "\n",
    "- Note that all partial derivatives are computed simultaniously.\n",
    "\n",
    "Here is the code to compute the gradient of the cost function with multiple variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5b0fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function gradient calculator\n",
    "def compute_cost_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient of the cost function based on the linear regression model with multiple variables\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           # (number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):                             \n",
    "        diff = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] += diff * X[i, j]    \n",
    "        dj_db += diff                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7513fe4",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "In linear regression with multiple variables, the linear model that predicts $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is given as:\n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\nonumber $$\n",
    "\n",
    "We utilize input training data to fit the parameters $\\mathbf{w}$, $b$ by minimizing a measure of the error between our predictions $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(\\mathbf{w},b)$. In training, we measure the cost over all of our training samples $\\mathbf{x}^{(i)},y^{(i)}$ by:\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\nonumber $$ \n",
    "\n",
    "Using the *gradient descent* method, parameters $\\mathbf{w}$, $b$ are updated simultaneously as follows:\n",
    "\n",
    "\\begin{align}\n",
    "w_{j} &= w_{j} - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_{j}} \\qquad \\forall j = 0, \\dots, n-1 \\nonumber \\\\ \n",
    "b &= b - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ is the learing rate.\n",
    "\n",
    "- We repeat the update precedure until the convergence criteria are met. \n",
    "- Note that when the gradient is negative, $\\mathbf{w}$ or $b$ is decreasing and vice-versa.\n",
    "\n",
    "Here is the code for gradient decent method with multiple variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a18425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient decent method\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, rel_err): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      alpha (float)       : learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      rel_err(float)      : relative error in the gradient decent\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar)      : Updated value of parameter after running gradient descent\n",
    "      b (scalar)      : Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    b         = b_in\n",
    "    w         = w_in\n",
    "    rel_diff  = 1.0\n",
    "    i         = 0\n",
    "    \n",
    "    # Loop over number of iterations\n",
    "    while (i < num_iters) or (rel_diff > rel_err):\n",
    "        \n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = compute_cost_gradient(X, y, w, b)\n",
    "\n",
    "        # Update Parameters using equation for the gradient decent\n",
    "        w = w - alpha * dj_dw                            \n",
    "        b = b - alpha * dj_db                           \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000: # prevent resource exhaustion \n",
    "            J_history.append(compute_cost(X, y, w, b))\n",
    "        # Relative difference\n",
    "        if i > 0:\n",
    "            rel_diff = abs(J_history[i]-J_history[i-1])/J_history[i]\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Itr {i:4}: Cost = {J_history[-1]:8.9f}, rel_diff = {rel_diff:0.9e}\")\n",
    "            \n",
    "        # Update\n",
    "        i += 1\n",
    " \n",
    "    # return w and J,w history for graphing\n",
    "    print(f\"Itr {i:4}: Cost = {J_history[-1]:8.9f}, rel_diff = {rel_diff:0.9e}\")\n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc26135",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "Feature scaling, essentially dividing each positive feature by its maximum value, or more generally, rescale each feature by both its minimum and maximum values using (x-min)/(max-min). Both ways normalizes features to the range of -1 and 1, where the former method works for positive features which is simple and the latter method works for any features. There are two recommended feature scaling methods:\n",
    "\n",
    "- Mean ($\\mu$) normalization: \n",
    "\\begin{align}\n",
    "    x_i = \\frac{x_i - \\mu_i}{max - min} \\nonumber\n",
    "\\end{align}\n",
    "- Z-score normalization as discussed below. \n",
    "\n",
    "## z-score normalization \n",
    "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "To implement z-score normalization, adjust your input values by using this formula:\n",
    "\n",
    "\\begin{align}\n",
    "    x^{(i)}_j = \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5} \\nonumber \\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "- Implementation Note: When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters from the model, we often want to predict the new input we have not seen before. Given a new $x$ value, we must first normalize $x$ using the mean and standard deviation that we had previously computed from the training set.\n",
    "\n",
    "Here is the code for feature scaling using z-score normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbf726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    " \n",
    "# from sklearn.preprocessing import scale\n",
    "# scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537dc93",
   "metadata": {},
   "source": [
    "# Plot cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc92a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(J_hist):\n",
    "    # plot cost versus iteration  \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "    ax1.plot(J_hist)\n",
    "    ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "    ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "    ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "    ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac772db",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245d4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cost(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression with multiple variable\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdcbd9",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The training dataset contains three examples (housing price) with four features (size, bedrooms, floors and, age) shown in the table below. \n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75dd28",
   "metadata": {},
   "source": [
    "# Plot problem dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfa2963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load our data set\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']\n",
    "    \n",
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b17e8d",
   "metadata": {},
   "source": [
    "The plot below shows steps involved in Z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae1d14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu     = np.mean(X_train,axis=0)   \n",
    "sigma  = np.std(X_train,axis=0) \n",
    "X_mean = (X_train - mu)\n",
    "X_norm = (X_train - mu)/sigma      \n",
    "\n",
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"unnormalized\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Z-score normalized\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aab548",
   "metadata": {},
   "source": [
    "The plot above shows the relationship between two of the training set parameters, \"age\" and \"size(sqft)\". *These are plotted with equal scale*. \n",
    "- Left: Unnormalized: The range of values or the variance of the 'size (sqft)' feature is much larger than that of age\n",
    "- Middle: The first step removes the mean or average value from each feature. This leaves features that are centered around zero. It's difficult to see the difference for the 'age' feature, but 'size (sqft)' is clearly around zero.\n",
    "- Right: The second step divides by the variance. This leaves both features centered at zero with a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d28ffe",
   "metadata": {},
   "source": [
    "# Run example with gradient descent and sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6dede3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr    0: Cost = 48883.733832062, rel_diff = 1.000000000e+00\n",
      "Itr  100: Cost = 15742.217933301, rel_diff = 1.051826686e-02\n",
      "Itr  200: Cost = 5668.534605092, rel_diff = 1.013651444e-02\n",
      "Itr  300: Cost = 2074.670978492, rel_diff = 1.008431454e-02\n",
      "Itr  400: Cost = 761.002571748, rel_diff = 1.007730290e-02\n",
      "Itr  500: Cost = 279.223350310, rel_diff = 1.007635873e-02\n",
      "Itr  600: Cost = 102.455388819, rel_diff = 1.007622697e-02\n",
      "Itr  700: Cost = 37.594165218, rel_diff = 1.007620400e-02\n",
      "Itr  800: Cost = 13.794522059, rel_diff = 1.007619563e-02\n",
      "Itr  900: Cost = 5.061662379, rel_diff = 1.007618925e-02\n",
      "Itr 1000: Cost = 1.857291001, rel_diff = 1.007618320e-02\n",
      "Itr 1100: Cost = 0.681501779, rel_diff = 1.007617723e-02\n",
      "Itr 1200: Cost = 0.250065793, rel_diff = 1.007617131e-02\n",
      "Itr 1300: Cost = 0.091757556, rel_diff = 1.007616545e-02\n",
      "Itr 1400: Cost = 0.033668955, rel_diff = 1.007615964e-02\n",
      "Itr 1500: Cost = 0.012354287, rel_diff = 1.007615388e-02\n",
      "Itr 1600: Cost = 0.004533211, rel_diff = 1.007614817e-02\n",
      "Itr 1700: Cost = 0.001663392, rel_diff = 1.007614251e-02\n",
      "Itr 1800: Cost = 0.000610356, rel_diff = 1.007613690e-02\n",
      "Itr 1900: Cost = 0.000223961, rel_diff = 1.007613134e-02\n",
      "Itr 2000: Cost = 0.000082179, rel_diff = 1.007612583e-02\n",
      "Itr 2100: Cost = 0.000030154, rel_diff = 1.007612037e-02\n",
      "Itr 2200: Cost = 0.000011065, rel_diff = 1.007611495e-02\n",
      "Itr 2300: Cost = 0.000004060, rel_diff = 1.007610958e-02\n",
      "Itr 2400: Cost = 0.000001490, rel_diff = 1.007610425e-02\n",
      "Itr 2500: Cost = 0.000000547, rel_diff = 1.007609898e-02\n",
      "Itr 2600: Cost = 0.000000201, rel_diff = 1.007609372e-02\n",
      "Itr 2700: Cost = 0.000000074, rel_diff = 1.007608863e-02\n",
      "Itr 2800: Cost = 0.000000027, rel_diff = 1.007608357e-02\n",
      "Itr 2900: Cost = 0.000000010, rel_diff = 1.007607822e-02\n",
      "Itr 3000: Cost = 0.000000004, rel_diff = 1.007607224e-02\n",
      "Itr 3100: Cost = 0.000000001, rel_diff = 1.007606858e-02\n",
      "Itr 3200: Cost = 0.000000000, rel_diff = 1.007606220e-02\n",
      "Itr 3300: Cost = 0.000000000, rel_diff = 1.007605844e-02\n",
      "Itr 3400: Cost = 0.000000000, rel_diff = 1.007605173e-02\n",
      "Itr 3500: Cost = 0.000000000, rel_diff = 1.007605058e-02\n",
      "Itr 3600: Cost = 0.000000000, rel_diff = 1.007605412e-02\n",
      "Itr 3700: Cost = 0.000000000, rel_diff = 1.007602629e-02\n",
      "Itr 3800: Cost = 0.000000000, rel_diff = 1.007599136e-02\n",
      "Itr 3900: Cost = 0.000000000, rel_diff = 1.007593110e-02\n",
      "Itr 4000: Cost = 0.000000000, rel_diff = 1.007608566e-02\n",
      "Itr 4100: Cost = 0.000000000, rel_diff = 1.007589077e-02\n",
      "Itr 4200: Cost = 0.000000000, rel_diff = 1.007641637e-02\n",
      "Itr 4300: Cost = 0.000000000, rel_diff = 1.007650458e-02\n",
      "Itr 4400: Cost = 0.000000000, rel_diff = 1.007720996e-02\n",
      "Itr 4500: Cost = 0.000000000, rel_diff = 1.007610500e-02\n",
      "Itr 4600: Cost = 0.000000000, rel_diff = 1.007580608e-02\n",
      "Itr 4700: Cost = 0.000000000, rel_diff = 1.007627121e-02\n",
      "Itr 4800: Cost = 0.000000000, rel_diff = 1.008288705e-02\n",
      "Itr 4900: Cost = 0.000000000, rel_diff = 1.006500254e-02\n",
      "Itr 5000: Cost = 0.000000000, rel_diff = 1.008493076e-02\n",
      "Itr 5100: Cost = 0.000000000, rel_diff = 1.009873306e-02\n",
      "Itr 5200: Cost = 0.000000000, rel_diff = 1.004175055e-02\n",
      "Itr 5250: Cost = 0.000000000, rel_diff = 9.988148806e-03\n",
      "(J, w, b) found by gradient descent: (0.0000000, [ 38.05161505  41.54327451 -30.98894656  36.34177447], 290.0000000)\n",
      "--------------------------------------------------------------------\n",
      "i:  0, prediction: 460.000000, target value: 460\n",
      "i:  1, prediction: 232.000000, target value: 232\n",
      "i:  2, prediction: 178.000000, target value: 178\n",
      "--------------------------------------------------------------------\n",
      "SGDRegressor(alpha=0.005, max_iter=10000)\n",
      "(Sklearn) number of iterations completed: 1346, number of weight updates: 4039.0\n",
      "model parameters:                   w: [ 37.99574539  41.46688625 -30.8275706   36.29504129], b:[289.66999503]\n",
      "prediction using np.dot() and sgdr.predict match: True\n",
      "Prediction on training set:\n",
      "[459.32600087 231.92314137 177.76084286]\n",
      "Target values \n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # initialize parameters\n",
    "    w_init = np.zeros(4,)\n",
    "    b_init = 0.0\n",
    "\n",
    "    # some gradient descent settings\n",
    "    num_iters = 1000\n",
    "    alpha     = 5.0e-3\n",
    "    rel_err   = 1.0e-2\n",
    "    \n",
    "    # normalize the original features\n",
    "    X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "    \n",
    "    # run gradient descent\n",
    "    w_final, b_final, J_hist = gradient_descent(X_norm, y_train, w_init, b_init, alpha, num_iters, rel_err)\n",
    "    print(f\"(J, w, b) found by gradient descent: ({J_hist[-1]:0.7f}, {w_final}, {b_final:0.7f})\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    \n",
    "    m,_ = X_norm.shape\n",
    "    for i in range(m):\n",
    "        print(f\"i: {i:2d}, prediction: {predict_cost(X_norm[i], w_final, b_final):0.6f}, target value: {y_train[i]}\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "        \n",
    "    # Sklearn normalize the orignal features\n",
    "    scaler = StandardScaler()\n",
    "    X_norm_Sklearn = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Create and fit the regression model (Sklearn)\n",
    "    sgdr = SGDRegressor(max_iter=10000, alpha=5.0e-3)\n",
    "    sgdr.fit(X_norm_Sklearn, y_train)\n",
    "    print(sgdr)\n",
    "    print(f\"(Sklearn) number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n",
    "    \n",
    "    # View parameters\n",
    "    b_norm_sklearn = sgdr.intercept_\n",
    "    w_norm_sklearn = sgdr.coef_\n",
    "    print(f\"model parameters:                   w: {w_norm_sklearn}, b:{b_norm_sklearn}\")\n",
    "    \n",
    "    # make a prediction using sgdr.predict()\n",
    "    y_pred_sgd = sgdr.predict(X_norm_Sklearn)\n",
    "    # make a prediction using w,b. \n",
    "    y_pred = np.dot(X_norm_Sklearn, w_norm_sklearn) + b_norm_sklearn  \n",
    "    print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")\n",
    "\n",
    "    print(f\"Prediction on training set:\\n{y_pred[:4]}\" )\n",
    "    print(f\"Target values \\n{y_train[:4]}\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2501d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
